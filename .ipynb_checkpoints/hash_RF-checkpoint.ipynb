{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections as col\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import math\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(name, vocabulary_size):\n",
    "    train_data = pd.read_csv(name)\n",
    "    raw_data = train_data.text.values\n",
    "    X_label = np.array([0 if i < 0 else 1 for i in train_data.label.values])\n",
    "    x = len(raw_data)\n",
    "    X_train = np.zeros((x, 150))\n",
    "    for i in range(x):\n",
    "        words = nltk.word_tokenize(raw_data[i])\n",
    "        y = len(words)\n",
    "        for j in range(y):\n",
    "            X_train[i][150-y+j] = hash(words[j]) % vocabulary_size\n",
    "    return X_train, X_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xTr, yTr = get_data('train.csv',5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1089, 150)\n",
      "[    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.  3540.\n",
      "  2543.  3703.    81.  2852.  4252.  4468.   643.  1506.   784.  2001.\n",
      "  4970.  2136.   155.   160.    87.  1631.  3730.  4663.  2522.  2483.]\n"
     ]
    }
   ],
   "source": [
    "# len(x_train[0])\n",
    "# print(xTr.shape)\n",
    "# print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train.csv',newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    i=0\n",
    "    link = \"https://\"\n",
    "    xTr = np.ones((1089,4))\n",
    "    yTr = []\n",
    "    for row in spamreader:\n",
    "        #only take id,text and label\n",
    "        if i !=0:\n",
    "            #dic.update({row[0]:[row[1],row[-1]]})\n",
    "            # the first feature is whether text has link or not\n",
    "            if link in row[1]:\n",
    "                xTr[i-1][3] = 0.8\n",
    "            else:\n",
    "                xTr[i-1][3] = 0.1\n",
    "#             xTr[i-1][3] = row[3] # the second feature is favoriteCount\n",
    "            #the third feature is time\n",
    "            xTr[i-1][1] = float(row[5].split(' ')[1].split(':')[0]) \n",
    "            xTr[i-1][2] = float(row[5].split(' ')[1].split(':')[1]) \n",
    "#             xTr[i-1][4] =  row[12] #the forth feature is retweetCount \n",
    "            yTr.append(int(row[-1]))\n",
    "        i += 1\n",
    "    yTr = np.array(yTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('train.csv',newline='') as csvfile:\n",
    "#     spamreader = csv.reader(csvfile)\n",
    "#     i=0\n",
    "#     link = \"https://\"\n",
    "#     xTr = np.ones((1089,3))\n",
    "#     yTr = []\n",
    "#     for row in spamreader:\n",
    "#         #only take id,text and label\n",
    "#         if i !=0:\n",
    "#             #dic.update({row[0]:[row[1],row[-1]]})\n",
    "#             # the first feature is whether text has link or not\n",
    "# #             if link in row[1]:\n",
    "# #                 xTr[i-1][2] = 1\n",
    "# #             else:\n",
    "# #                 xTr[i-1][2] = 0\n",
    "# #             xTr[i-1][3] = row[3] # the second feature is favoriteCount\n",
    "#             #the third feature is time\n",
    "#             xTr[i-1][1] = float(row[5].split(' ')[1].split(':')[0]) \n",
    "#             xTr[i-1][2] = float(row[5].split(' ')[1].split(':')[1]) \n",
    "# #             xTr[i-1][4] =  row[12] #the forth feature is retweetCount \n",
    "#             yTr.append(int(row[-1]))\n",
    "#         i += 1\n",
    "#     yTr = np.array(yTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTr[:,1] = xTr[:,1]/np.amax(xTr[:,1])\n",
    "xTr[:,2] = xTr[:,2]/np.amax(xTr[:,2])\n",
    "# xTr[:,3] = xTr[:,3]/np.amax(xTr[:,3])\n",
    "# xTr[:,4] = xTr[:,4]/np.amax(xTr[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_rnn.csv',newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    i = 0\n",
    "    for row in spamreader:\n",
    "        #only take id,text and label\n",
    "        if i !=0:\n",
    "            xTr[i-1][0] = row[1]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.29434255,  0.95652174,  0.30508475,  0.8       ])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTr[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(yTr)\n",
    "split = int(n*0.8)\n",
    "x_Tr = xTr[:split,:]\n",
    "y_Tr = yTr[:split]\n",
    "x_Vali = xTr[split:,:]\n",
    "y_Vali = yTr[split:]\n",
    "split_2 = int(n*0.1)\n",
    "x_v = xTr[:split_2,:]\n",
    "y_v = yTr[:split_2]\n",
    "x_t = xTr[split_2:,:]\n",
    "y_t = yTr[split_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set:  981\n",
      "validation set:  108\n"
     ]
    }
   ],
   "source": [
    "print('training set: ',len(x_t))\n",
    "print('validation set: ',len(x_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# depth = [2,3,4,5]\n",
    "# accuracy = []\n",
    "# for i in range(len(depth)):\n",
    "#     clf = RandomForestClassifier(n_estimators = 50, max_depth = depth[i], random_state = 0)\n",
    "#     clf.fit(x_t,y_t)\n",
    "#     out = clf.predict(x_v)\n",
    "#     error = len(np.where((out+y_v) == 0))/len(y_v)\n",
    "#     accuracy.append((100-error*100))\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.866820276498\n"
     ]
    }
   ],
   "source": [
    "# tree_number = [500,510,520,530,540,550]\n",
    "# depth = [1,2,3,4]\n",
    "# for i in depth:\n",
    "# clf = RandomForestClassifier(bootstrap=True, n_estimators = 500,max_features=1, max_depth =3,random_state = 0)\n",
    "clf = RandomForestClassifier(bootstrap=True, n_estimators = 500,max_features=2, max_depth =3,random_state = 0)\n",
    "scores = cross_val_score(clf, xTr, yTr, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=3, max_features=1, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(xTr,yTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test.csv',newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    i=0\n",
    "    #dic={}\n",
    "    link = \"https://\"\n",
    "    xTe = np.ones((300,3))\n",
    "    for row in spamreader:\n",
    "        #only take id,text and label\n",
    "        if i != 0:\n",
    "            if link in row[1]:\n",
    "                xTe[i-1][2] = 1\n",
    "            else:\n",
    "                xTe[i-1][2] = 0\n",
    "#             xTe[i-1][1] = row[3] # the second feature is favoriteCount\n",
    "            #the third feature is time\n",
    "            xTe[i-1][1] = int(row[5].split(' ')[1].split(':')[0]) \n",
    "#             xTe[i-1][3] = int(row[5].split(' ')[1].split(':')[1]) \n",
    "#             xTe[i-1][4] =  row[11] #the forth feature is retweetCount \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTe[:,1] = xTe[:,1]/np.amax(xTe[:,1])\n",
    "xTe[:,2] = xTe[:,2]/np.amax(xTe[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test _rnn.csv',newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    i = 0\n",
    "    for row in spamreader:\n",
    "        #only take id,text and label\n",
    "        if i !=0:\n",
    "            xTe[i-1][0] = row[1]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65228456,  0.65217391,  0.        ])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65228456  0.65217391  0.        ]\n",
      "[-1 -1 -1  1  1  1 -1 -1  1 -1 -1  1  1  1 -1 -1 -1 -1 -1  1  1 -1  1  1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1 -1  1  1  1 -1\n",
      " -1  1 -1  1  1  1 -1 -1  1  1 -1  1  1 -1 -1 -1  1  1  1 -1 -1 -1 -1 -1 -1\n",
      "  1 -1  1 -1  1  1  1  1  1 -1  1 -1  1  1  1  1  1  1 -1  1  1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  1  1  1  1 -1 -1 -1 -1 -1 -1  1 -1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1 -1 -1 -1  1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1 -1 -1 -1\n",
      " -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1 -1 -1 -1  1  1  1 -1 -1\n",
      "  1 -1  1 -1 -1 -1 -1 -1 -1  1  1  1  1 -1 -1 -1  1  1  1 -1 -1  1 -1  1  1\n",
      " -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1 -1 -1  1 -1 -1 -1\n",
      "  1 -1 -1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(xTe[0])\n",
    "predicted = clf.predict(xTe)\n",
    "print(predicted)\n",
    "ID = np.arange(0,len(predicted)+1)\n",
    "data = zip(ID,predicted)\n",
    "with open('result_RF_rnn_normal.csv','w',newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter.writerow([\"ID\",\"Label\"])\n",
    "    for d in data:\n",
    "        spamwriter.writerow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
